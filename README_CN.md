# Free Chat

**æ‹’ç»åºŸè¯ï¼ŒåŸºäºå¾®æœåŠ¡çš„ LLM èŠå¤©å¹³å°ã€‚**
Go åç«¯ï¼ŒPython æ¨ç†ï¼Œæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²ã€‚

[English](README.md) | [ä¸­æ–‡](README_CN.md)

## ğŸ— æ¶æ„

æ ‡å‡†å¾®æœåŠ¡æ¨¡å¼ã€‚æ²¡æœ‰é­”æ³•ï¼Œåªæœ‰ç¡¬æ ¸å·¥ç¨‹ã€‚

```mermaid
graph TD
    User((ç”¨æˆ·)) -->|HTTP| WebUI[Web UI / Nginx]
    User -->|REST| Gateway[API Gateway]
    
    subgraph "æ§åˆ¶å¹³é¢ (Control Plane)"
        Gateway -->|gRPC| Auth[Auth Service]
        Gateway -->|gRPC| Chat[Chat Service]
        Auth --> DB[(PostgreSQL)]
        Chat --> DB
        Chat --> Redis[(Redis)]
        Chat --> MQ[RocketMQ]
    end
    
    subgraph "è®¡ç®—å¹³é¢ (Compute Plane)"
        Chat -->|gRPC| LLM[LLM Inference Service]
    end
    
    Consul[Consul æœåŠ¡æ³¨å†Œ] -.->|Register/Discover| Gateway
    Consul -.->|Register| Auth
    Consul -.->|Register| Chat
    Consul -.->|Register| LLM
```

## ğŸ”„ æ•°æ®æµ

èŠå¤©æ¶ˆæ¯çš„è¯·æ±‚è·¯å¾„ã€‚çº¯ SSE æµå¼ä¼ è¾“ã€‚

```mermaid
sequenceDiagram
    participant U as User
    participant G as API Gateway
    participant C as Chat Service
    participant L as LLM Service
    participant M as RocketMQ
    
    U->>G: POST /chat/message
    G->>C: gRPC SendMessage
    
    %% Async Persistence
    par å¼‚æ­¥æŒä¹…åŒ–
        C->>M: å‘å¸ƒ "save-message"
    and å®æ—¶æ¨ç†
        C->>L: gRPC StreamInference
        
        loop Token ç”Ÿæˆ
            L->>C: æµå¼å“åº” (Token)
            C->>G: gRPC æµå¼å“åº”
            G->>U: SSE äº‹ä»¶ (Token)
        end
    end
    
    %% Final Save
    C->>M: å‘å¸ƒ "save-assistant-message"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å•èŠ‚ç‚¹ (å¼€å‘)
ç»å…¸æ–¹å¼ã€‚åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œæ‰€æœ‰å†…å®¹ã€‚

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/einspanner/free-chat.git
cd free-chat

# è¿è¡Œ
docker compose up -d --build
```

è®¿é—®åœ°å€: `http://localhost:3000`

### 2. åˆ†å¸ƒå¼éƒ¨ç½² (ç”Ÿäº§å°±ç»ª)
å°†å¤§è„‘ï¼ˆæ§åˆ¶å¹³é¢ï¼‰ä¸è‚Œè‚‰ï¼ˆGPU è®¡ç®—ï¼‰åˆ†ç¦»ã€‚

**æœåŠ¡å™¨ A (æ§åˆ¶å¹³é¢):**
è¿è¡Œ Gateway, Auth, DB, MQ, Consulã€‚
```bash
export ADVERTISE_IP=100.100.1.1  # æœåŠ¡å™¨ A çš„ Tailscale/å±€åŸŸç½‘ IP
docker-compose -f docker-compose-control.yml up -d
```

**æœåŠ¡å™¨ B (GPU è®¡ç®—):**
è¿è¡Œ Chat Service, LLM Inferenceã€‚
```bash
export ADVERTISE_IP=100.100.1.2  # æœåŠ¡å™¨ B çš„ Tailscale/å±€åŸŸç½‘ IP
export CONTROL_PLANE_IP=100.100.1.1 # è¿æ¥åˆ°æœåŠ¡å™¨ A
docker-compose -f docker-compose-compute.yml up -d
```

### 3. è¿è¡Œ Qwen-3B (é«˜æ€§èƒ½)
å¦‚æœä½ æœ‰æ˜¾å­˜ï¼Œåˆ«å‡‘åˆç”¨ 0.6B å°æ¨¡å‹ã€‚

**æ–¹æ³• Aï¼šç¯å¢ƒå˜é‡ (æ¨è)**
ä¿®æ”¹ `docker-compose.yml` æˆ–åœ¨ export å‘½ä»¤ä¸­æŒ‡å®šï¼š
```bash
export MODEL_NAME="Qwen/Qwen2.5-3B-Instruct"
```

**æ–¹æ³• Bï¼šDocker Compose è¦†ç›–**
```yaml
  llm-inference:
    environment:
      - MODEL_NAME=Qwen/Qwen2.5-3B-Instruct
```
*æ³¨æ„ï¼šè¿è¡Œ 3B æ¨¡å‹ç¡®ä¿ä½ çš„ GPU è‡³å°‘æœ‰ 8GB æ˜¾å­˜ã€‚*

## ğŸ›  æŠ€æœ¯æ ˆ
- **Go**: é«˜å¹¶å‘æœåŠ¡ (Gateway, Auth, Chat)ã€‚
- **Python**: PyTorch/HuggingFace æ¨ç†ã€‚
- **gRPC**: ä½å»¶è¿ŸæœåŠ¡é—´é€šä¿¡ã€‚
- **RocketMQ**: å¼‚æ­¥æ¶ˆæ¯æŒä¹…åŒ–ã€‚
- **Consul**: åŠ¨æ€æœåŠ¡å‘ç°ã€‚
- **Tailscale**: åˆ†å¸ƒå¼èŠ‚ç‚¹çš„å®‰å…¨ç½‘çŠ¶ç½‘ç»œã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
.
â”œâ”€â”€ cmd/                # å…±äº«å‘½ä»¤è¡Œå·¥å…·
â”œâ”€â”€ config/             # å…¨å±€é…ç½®æ–‡ä»¶
â”œâ”€â”€ deploy/             # éƒ¨ç½²é…ç½® (ä¾‹å¦‚ HF Spaces)
â”œâ”€â”€ pkg/                # Shared Go packages (Proto, Utils)
â”œâ”€â”€ services/           # å¾®æœåŠ¡æºç 
â”‚   â”œâ”€â”€ api-gateway/    # HTTP ç½‘å…³
â”‚   â”œâ”€â”€ auth-service/   # è®¤è¯æœåŠ¡
â”‚   â”œâ”€â”€ chat-service/   # èŠå¤©ä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ llm-inference/  # Python LLM æœåŠ¡
â”‚   â””â”€â”€ web-ui/         # å‰ç«¯é™æ€æ–‡ä»¶
â””â”€â”€ docker-compose.yml  # æœ¬åœ°å¼€å‘ç¼–æ’
```
