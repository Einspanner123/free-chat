// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        v3.21.12
// source: llm_inference.proto

package inference

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type Message struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Role          string                 `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Message) Reset() {
	*x = Message{}
	mi := &file_llm_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Message) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Message) ProtoMessage() {}

func (x *Message) ProtoReflect() protoreflect.Message {
	mi := &file_llm_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Message.ProtoReflect.Descriptor instead.
func (*Message) Descriptor() ([]byte, []int) {
	return file_llm_inference_proto_rawDescGZIP(), []int{0}
}

func (x *Message) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *Message) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

type InferenceRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	SessionId     string                 `protobuf:"bytes,1,opt,name=session_id,json=sessionId,proto3" json:"session_id,omitempty"` // 输入文本
	UserQuery     string                 `protobuf:"bytes,2,opt,name=user_query,json=userQuery,proto3" json:"user_query,omitempty"`
	History       []*Message             `protobuf:"bytes,3,rep,name=history,proto3" json:"history,omitempty"`
	Model         string                 `protobuf:"bytes,4,opt,name=model,proto3" json:"model,omitempty"`
	Temperature   float32                `protobuf:"fixed32,5,opt,name=temperature,proto3" json:"temperature,omitempty"`
	MaxTokens     int32                  `protobuf:"varint,6,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	StopWords     []string               `protobuf:"bytes,7,rep,name=stop_words,json=stopWords,proto3" json:"stop_words,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceRequest) Reset() {
	*x = InferenceRequest{}
	mi := &file_llm_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceRequest) ProtoMessage() {}

func (x *InferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceRequest.ProtoReflect.Descriptor instead.
func (*InferenceRequest) Descriptor() ([]byte, []int) {
	return file_llm_inference_proto_rawDescGZIP(), []int{1}
}

func (x *InferenceRequest) GetSessionId() string {
	if x != nil {
		return x.SessionId
	}
	return ""
}

func (x *InferenceRequest) GetUserQuery() string {
	if x != nil {
		return x.UserQuery
	}
	return ""
}

func (x *InferenceRequest) GetHistory() []*Message {
	if x != nil {
		return x.History
	}
	return nil
}

func (x *InferenceRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *InferenceRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *InferenceRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *InferenceRequest) GetStopWords() []string {
	if x != nil {
		return x.StopWords
	}
	return nil
}

type InferenceResponse struct {
	state           protoimpl.MessageState `protogen:"open.v1"`
	Token           string                 `protobuf:"bytes,1,opt,name=token,proto3" json:"token,omitempty"`
	IsFinished      bool                   `protobuf:"varint,2,opt,name=is_finished,json=isFinished,proto3" json:"is_finished,omitempty"`
	Error           string                 `protobuf:"bytes,3,opt,name=error,proto3" json:"error,omitempty"`
	GeneratedTokens int32                  `protobuf:"varint,4,opt,name=generated_tokens,json=generatedTokens,proto3" json:"generated_tokens,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *InferenceResponse) Reset() {
	*x = InferenceResponse{}
	mi := &file_llm_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceResponse) ProtoMessage() {}

func (x *InferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceResponse.ProtoReflect.Descriptor instead.
func (*InferenceResponse) Descriptor() ([]byte, []int) {
	return file_llm_inference_proto_rawDescGZIP(), []int{2}
}

func (x *InferenceResponse) GetToken() string {
	if x != nil {
		return x.Token
	}
	return ""
}

func (x *InferenceResponse) GetIsFinished() bool {
	if x != nil {
		return x.IsFinished
	}
	return false
}

func (x *InferenceResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *InferenceResponse) GetGeneratedTokens() int32 {
	if x != nil {
		return x.GeneratedTokens
	}
	return 0
}

var File_llm_inference_proto protoreflect.FileDescriptor

const file_llm_inference_proto_rawDesc = "" +
	"\n" +
	"\x13llm_inference.proto\x12\tinference\"7\n" +
	"\aMessage\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\"\xf4\x01\n" +
	"\x10InferenceRequest\x12\x1d\n" +
	"\n" +
	"session_id\x18\x01 \x01(\tR\tsessionId\x12\x1d\n" +
	"\n" +
	"user_query\x18\x02 \x01(\tR\tuserQuery\x12,\n" +
	"\ahistory\x18\x03 \x03(\v2\x12.inference.MessageR\ahistory\x12\x14\n" +
	"\x05model\x18\x04 \x01(\tR\x05model\x12 \n" +
	"\vtemperature\x18\x05 \x01(\x02R\vtemperature\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x06 \x01(\x05R\tmaxTokens\x12\x1d\n" +
	"\n" +
	"stop_words\x18\a \x03(\tR\tstopWords\"\x8b\x01\n" +
	"\x11InferenceResponse\x12\x14\n" +
	"\x05token\x18\x01 \x01(\tR\x05token\x12\x1f\n" +
	"\vis_finished\x18\x02 \x01(\bR\n" +
	"isFinished\x12\x14\n" +
	"\x05error\x18\x03 \x01(\tR\x05error\x12)\n" +
	"\x10generated_tokens\x18\x04 \x01(\x05R\x0fgeneratedTokens2c\n" +
	"\x11InferencerService\x12N\n" +
	"\x0fStreamInference\x12\x1b.inference.InferenceRequest\x1a\x1c.inference.InferenceResponse0\x01B\x17Z\x15./inference;inferenceb\x06proto3"

var (
	file_llm_inference_proto_rawDescOnce sync.Once
	file_llm_inference_proto_rawDescData []byte
)

func file_llm_inference_proto_rawDescGZIP() []byte {
	file_llm_inference_proto_rawDescOnce.Do(func() {
		file_llm_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llm_inference_proto_rawDesc), len(file_llm_inference_proto_rawDesc)))
	})
	return file_llm_inference_proto_rawDescData
}

var file_llm_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 3)
var file_llm_inference_proto_goTypes = []any{
	(*Message)(nil),           // 0: inference.Message
	(*InferenceRequest)(nil),  // 1: inference.InferenceRequest
	(*InferenceResponse)(nil), // 2: inference.InferenceResponse
}
var file_llm_inference_proto_depIdxs = []int32{
	0, // 0: inference.InferenceRequest.history:type_name -> inference.Message
	1, // 1: inference.InferencerService.StreamInference:input_type -> inference.InferenceRequest
	2, // 2: inference.InferencerService.StreamInference:output_type -> inference.InferenceResponse
	2, // [2:3] is the sub-list for method output_type
	1, // [1:2] is the sub-list for method input_type
	1, // [1:1] is the sub-list for extension type_name
	1, // [1:1] is the sub-list for extension extendee
	0, // [0:1] is the sub-list for field type_name
}

func init() { file_llm_inference_proto_init() }
func file_llm_inference_proto_init() {
	if File_llm_inference_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llm_inference_proto_rawDesc), len(file_llm_inference_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   3,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_llm_inference_proto_goTypes,
		DependencyIndexes: file_llm_inference_proto_depIdxs,
		MessageInfos:      file_llm_inference_proto_msgTypes,
	}.Build()
	File_llm_inference_proto = out.File
	file_llm_inference_proto_goTypes = nil
	file_llm_inference_proto_depIdxs = nil
}
